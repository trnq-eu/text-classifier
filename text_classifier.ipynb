{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trnq-eu/text-classifier/blob/main/text_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV8v1haCxIch",
        "outputId": "2aca36d2-e964-4ea4-89d5-9cee7ca87621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy==3.* in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (8.1.9)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (1.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (4.65.0)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (1.1.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (3.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (23.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (1.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (3.1.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (1.10.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (2.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (3.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (2.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (67.7.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (2.27.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (2.4.6)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (6.3.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.*) (0.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy==3.*) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.*) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.*) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.*) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.*) (1.26.15)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy==3.*) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy==3.*) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy==3.*) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy==3.*) (2.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "2023-05-09 16:31:43.701789: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-09 16:31:45.625919: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting it-core-news-lg==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_lg-3.5.0/it_core_news_lg-3.5.0-py3-none-any.whl (567.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m567.9/567.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from it-core-news-lg==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (23.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->it-core-news-lg==3.5.0) (2.1.2)\n",
            "Installing collected packages: it-core-news-lg\n",
            "Successfully installed it-core-news-lg-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('it_core_news_lg')\n"
          ]
        }
      ],
      "source": [
        "!pip install -U spacy==3.*\n",
        "!pip install PyPDF2\n",
        "!python -m spacy download it_core_news_lg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import spacy\n",
        "import PyPDF2\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n"
      ],
      "metadata": {
        "id": "FL62jPySxbRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collegamento a Google Drive\n",
        "In questo modo è possibile esplorare documenti dalle proprie cartelle di Drive"
      ],
      "metadata": {
        "id": "Py1PZHSwn6JJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azF9gnj1oCXI",
        "outputId": "89619cd0-9a3c-47e1-9558-09241d22c655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# carica il corpus italiano di spacy (large)\n",
        "nlp = spacy.load('it_core_news_lg')\n",
        "\n",
        "# elenco delle stop word italiane\n",
        "italian_stop_words = list(nlp.Defaults.stop_words)"
      ],
      "metadata": {
        "id": "yRckydERRw3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carica i dati di training e crea una lista di testi e una lista di etichette corrispondenti. In questo caso le diverse cartelle di documenti sono state inserite all'interno di Google Drive in una cartella denominata \"DATI/comunicati_classificati/\"."
      ],
      "metadata": {
        "id": "tbWM2u6GQSwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definisci le etichette e le cartelle dei dati di training\n",
        "labels = ['Prodotto', 'Heritage', 'HR',  'Sustainability']\n",
        "folders = ['/content/drive/MyDrive/DATI/comunicati_classificati/Prodotto', \n",
        "           '/content/drive/MyDrive/DATI/comunicati_classificati/Heritage',\n",
        "           '/content/drive/MyDrive/DATI/comunicati_classificati/HR', \n",
        "           '/content/drive/MyDrive/DATI/comunicati_classificati/Sustainability']\n",
        "\n",
        "# Crea le liste vuote di testi e etichette\n",
        "texts = []\n",
        "labels_list = []\n",
        "\n",
        "# Loop attraverso le cartelle di training per caricare i testi e le etichette\n",
        "for i, folder in enumerate(folders):\n",
        "    for filename in os.listdir(folder):\n",
        "        with open(os.path.join(folder, filename), 'rb') as f:\n",
        "            pdf = PyPDF2.PdfReader(f)\n",
        "            text = ''\n",
        "            for page in pdf.pages:\n",
        "                text += page.extract_text()\n",
        "            texts.append(text)\n",
        "            labels_list.append(i)\n"
      ],
      "metadata": {
        "id": "k3XIQ-CcOBgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funzione di preprocessamento dei testi che elimina dai testi dei documenti tutte le parole inutili (*stop words*)."
      ],
      "metadata": {
        "id": "s8gIhZ7jzsCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# carica lo spacy model per l'italiano\n",
        "nlp = spacy.load('it_core_news_lg')\n",
        "\n",
        "# funzione di pre-elaborazione personalizzata\n",
        "def preprocess_text(text):\n",
        "    # rimuovi le stop words in italiano\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc if not token.is_stop]\n",
        "    # unisci i token in una stringa\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "00wLK0hdSJYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stampa le etichette delle classi\n",
        "print(labels_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em9etIJnQ7Wi",
        "outputId": "20a178c0-8bcf-417f-ca53-ae100aa999df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Di seguito otteniamo la vettorizzazione con TfidfVectorizer (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) e scegliamo il classificatore MultinomialNB (https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html), un classificatore Bayesiano particolarmente efficace per la classificazione multiclasse. \n"
      ],
      "metadata": {
        "id": "BaTrOBfyz_Ne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorizer = TfidfVectorizer(preprocessor=preprocess_text, ngram_range=(1, 2))\n",
        "vectorizer = TfidfVectorizer(preprocessor=preprocess_text)\n",
        "\n",
        "\n",
        "# definisce un classificatore\n",
        "clf = MultinomialNB()\n",
        "\n",
        "# definisce un pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', vectorizer),\n",
        "    ('clf', clf)\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "xn8M2Wf-Rbs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# Carica il modello di lingua italiana di Spacy\n",
        "nlp = spacy.load('it_core_news_lg')\n",
        "\n",
        "# Definisci una funzione per estrarre il testo da un file PDF\n",
        "def extract_text_from_pdf(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        pdf_reader = PyPDF2.PdfReader(f)\n",
        "        text = ''\n",
        "        for i in range(len(pdf_reader.pages)):\n",
        "            text += pdf_reader.pages[i].extract_text()\n",
        "    return text\n",
        "\n",
        "# Definisci una funzione per elaborare il testo utilizzando Spacy\n",
        "def preprocess_text(text):\n",
        "    doc = nlp(text)\n",
        "    # rimuovi le stop words e i simboli di punteggiatura\n",
        "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Definisci la pipeline di elaborazione e classificazione\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(preprocessor=preprocess_text)),\n",
        "    ('clf', LinearSVC())\n",
        "])\n",
        "\n",
        "# Crea un DataFrame vuoto per i dati di addestramento\n",
        "df = pd.DataFrame(columns=['text', 'category'])\n",
        "\n",
        "# Itera attraverso le cartelle dei dati di addestramento\n",
        "for category in ['Prodotto', 'Heritage', 'HR', 'Sustainability']:\n",
        "    category_path = os.path.join('/content/drive/MyDrive/DATI/comunicati_classificati', category)\n",
        "    for file_name in os.listdir(category_path):\n",
        "        if file_name.endswith('.pdf'):\n",
        "            file_path = os.path.join(category_path, file_name)\n",
        "            text = extract_text_from_pdf(file_path)\n",
        "            df = pd.concat([df, pd.DataFrame({'text': text, 'category': category}, index=[0])], ignore_index=True)\n",
        "\n",
        "\n",
        "# Addestra il modello\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['category'], test_size=0.1, random_state=42)\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Valuta il modello sui dati di test\n",
        "y_pred = pipeline.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Utilizza il modello per classificare i file PDF sconosciuti\n",
        "unknown_path = os.path.join('/content/drive/MyDrive/DATI/comunicati_classificati', 'Unknown')\n",
        "for file_name in os.listdir(unknown_path):\n",
        "    if file_name.endswith('.pdf'):\n",
        "        file_path = os.path.join(unknown_path, file_name)\n",
        "        text = extract_text_from_pdf(file_path)\n",
        "        category = pipeline.predict([text])[0]\n",
        "        print(f\"{file_name}: {category}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx30WM_YTiuE",
        "outputId": "ea75e598-3e9a-467a-acc2-684a8e5da022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                precision    recall  f1-score   support\n",
            "\n",
            "            HR       1.00      1.00      1.00         1\n",
            "      Heritage       1.00      0.50      0.67         2\n",
            "      Prodotto       0.00      0.00      0.00         0\n",
            "Sustainability       0.00      0.00      0.00         1\n",
            "\n",
            "      accuracy                           0.50         4\n",
            "     macro avg       0.50      0.38      0.42         4\n",
            "  weighted avg       0.75      0.50      0.58         4\n",
            "\n",
            "Nuova Abarth 500e elettrizza l'Europa _ Abarth _ Stellantis.pdf: Prodotto\n",
            "IT-20230428-Stellantis-2022-CSR-Report.pdf: Sustainability\n",
            "20221116_Stellantis_Names_New_Head_of_IR_IT.pdf: HR\n",
            "20230118-Stellantis-Board-Composition-Change-IT.pdf: HR\n",
            "Opel Vivaro-e e Opel Vivaro-e HYDROGEN.pdf: Prodotto\n",
            "Thierry Koskas è nominato Chief Executive Officer del marchio Citroën Stellantis.pdf: Sustainability\n",
            "Nuova PEUGEOT 408.pdf: Prodotto\n",
            "salone_auto_epoca.pdf: Heritage\n",
            "centenario_autodromo_monza.pdf: Heritage\n",
            "economia_circolare_mirafiori.pdf: Sustainability\n"
          ]
        }
      ]
    }
  ]
}